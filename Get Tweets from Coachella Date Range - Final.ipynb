{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Imoprt users list.  Broke up the ~125,000 valid users from the previous Jupyter notebook into 5 json files to help create more manageable datasets and to increase productivity with multiple people running the code without risking overlapping users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "folder = os.path.join('C:/', 'users', 'charr', 'documents')\n",
    "with open(os.path.join(folder, 'coachella_users_5.json'), mode = 'r', encoding = 'utf-8') as reader:\n",
    "    all_user_list = list(json.load(reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13077"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "from twython import TwythonError\n",
    "from twython import TwythonRateLimitError\n",
    "import dateutil.parser as dateparser\n",
    "from time import sleep\n",
    "from pprint import pprint\n",
    "\n",
    "consumer_key = 'Xs4Xzym3HuF5izdfCh6fbbbh6'\n",
    "consumer_secret = 'dyVs6OtGCqlSaBUAl4J1g0c9u29K5Xm3kKLee8O2Vvf2mo1T7P'\n",
    "access_token = '970138235514114048-jRtzTNNUGdGW7K1Uqvc48bok2zqpROa'\n",
    "access_token_secret = 'tMWPX4Uq1sQJNClWyLePprvllrY43ftNUV9o6AvrZt9Wb'\n",
    "\n",
    "\n",
    "twitter = Twython(consumer_key, consumer_secret, access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. using the timestamped snowflaked tweet ID as the max_id for all users you can start at the point in the user's tweet history when coachella ended to speed up the process (so rather than go back 200 tweets each request until we either did or did not reach the coachella time frame we cut that process out) and then can collect only the tweets that were in the created_at date range (so you don't go outside the coachella date range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "coachella_end_tweet_id = 1120182057383022594\n",
    "coachella_start_tweet_id = 1116479450026106880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_tweet_timestamp(tid):\n",
    "    offset = 1288834974657\n",
    "    tstamp = (tid >> 22) + offset\n",
    "    utcdttime = datetime.datetime.fromtimestamp(tstamp/1000, tz=datetime.timezone.utc)\n",
    "    return utcdttime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coachella_start_date = get_tweet_timestamp(1116479450026106880)\n",
    "coachella_loose_end_date = get_tweet_timestamp(1120182057383022594)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. functions work to get tweet history from each user.  We want to make sure that the tweet mode is 'extended' or else if the tweet is longer than 140 characters we won't get the whole tweet, and we can call a max of 200 tweets per request (1 request per second).  in the get_first_tweets_from_user() function, the max_id is set to the coachella end date tweet ID so we don't have to pull more tweets than necessary, and if we need to go back past 200 tweets for a single user the get_max_id() function feeds into the get_tweets_from_user() function. Since twitter requests are limited to 1 per second we also put a sleep timer after every call to guarantee that we won't exceed the request limit.  in the get_tweets() function we also added progress reports to check our progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_tweets_from_user(username):\n",
    "    initial_tweet_list =  twitter.get_user_timeline(screen_name = username, count = 200, tweet_mode='extended', max_id = coachella_end_tweet_id)\n",
    "    return initial_tweet_list\n",
    "\n",
    "def get_max_id(tweet_list):\n",
    "    maxid = tweet_list[-1]['id']\n",
    "    return maxid\n",
    "\n",
    "def get_tweets_from_user(username, maxid):\n",
    "    lst =  twitter.get_user_timeline(screen_name = username, count = 200, max_id = maxid, tweet_mode='extended')\n",
    "    return lst\n",
    "\n",
    "def get_tweet_time(tweet_dict):\n",
    "    return dateparser.parse(tweet_dict['created_at'])\n",
    "\n",
    "def get_tweet_id_time(tweet_dict):\n",
    "    return tweet_dict['id'], get_tweet_time(tweet_dict)\n",
    "\n",
    "def get_tweets(user, earliest_time, latest_time):\n",
    "    user_tweets = dict()\n",
    "    user_tweet_group = get_first_tweets_from_user(user)\n",
    "    \n",
    "    while True:\n",
    "        if not user_tweet_group:\n",
    "            print('3,200 tweet limit reached before', latest_time.strftime('%m/%d/%y'))\n",
    "            break\n",
    "        \n",
    "        min_id, min_datetime = get_tweet_id_time(user_tweet_group[-1])\n",
    "        if min_datetime > latest_time:\n",
    "            #print('Searching for tweet group,', min_datetime.strftime('%m/%d/%y'),'..')\n",
    "            print('.', sep = '', end = '')\n",
    "            \n",
    "        else:\n",
    "            max_id, max_datetime = get_tweet_id_time(user_tweet_group[0])\n",
    "            if max_datetime < earliest_time:\n",
    "                print('Found tweet before range', max_datetime.strftime('%m/%d/%y'))\n",
    "                break\n",
    "                \n",
    "            for tweet in user_tweet_group:\n",
    "                time = get_tweet_time(tweet)\n",
    "                if earliest_time <= time <= latest_time:\n",
    "                    user_tweets[tweet['id']] = tweet\n",
    "                    \n",
    "            if max_id == min_id:\n",
    "                break\n",
    "                \n",
    "        user_tweet_group = get_tweets_from_user(user, min_id)[1:]\n",
    "        sleep(1)\n",
    "\n",
    "    else:\n",
    "        print('3,200 tweet limit reached before', latest_time.strftime('%m/%d/%y'))\n",
    "        \n",
    "    sleep(1)\n",
    "    \n",
    "    return list(user_tweets.values())\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  ran the get_tweets() function on all users in teh all_user_list group.  once the length of the user_tweet_dct reaches 1000 the dict is exported to a json file, the old dict is cleared and the process is continued.  if we need to pause the query in the middle we also added a separate dictionary of all users who had been searched for previously  so that there is no risk of overlap.  We also kept the try/except catch in case any of the users had updated their status to private or been banned or anything else between the time we ran the original get_valid_users() function and now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######user_tweet_dct = {}\n",
    "#####finished_names = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweet_dict(dct):\n",
    "    file_suffix = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = 'coachella_tweets' + file_suffix + '.json'\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    with open(filepath, mode='w', encoding = 'utf-8') as f:\n",
    "        json.dump(dct, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unknown_users = [z for z in all_user_list if (z not in user_tweet_dct) and (z not in finished_names)]\n",
    "\n",
    "for user in unknown_users:\n",
    "    if len(user_tweet_dct) > 1000:\n",
    "        save_tweet_dict(user_tweet_dct)\n",
    "        user_tweet_dct = {}\n",
    "        \n",
    "    try:\n",
    "        user_tweet_dct[user] = get_tweets(user, coachella_start_date, coachella_loose_end_date)\n",
    "        finished_names[user] = True\n",
    "    except TwythonError:\n",
    "        user_tweet_dct[user] = []\n",
    "        finished_names[user] = True\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final save_tweet_dict for last group of <1000 users\n",
    "save_tweet_dict(user_tweet_dct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
